{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pymongo\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = Browser('chrome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    browser.visit(url)\n",
    "    html = browser.html    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://quotes.toscrape.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_quotes = []\n",
    "scraped_authors = []\n",
    "scraped_tags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### go to get_quotes2 ###\n",
    "\n",
    "# #Scrapes Quotes and Authors - DONE\n",
    "\n",
    "# scraped_quotes = []\n",
    "\n",
    "# def get_quotes(url):\n",
    "#     page_numbers = range(1,11)\n",
    "#     for page in page_numbers:\n",
    "#         soup = get_soup(f'http://quotes.toscrape.com/page/{page}')\n",
    "#         quotes = soup.find_all('div', class_='quote')\n",
    "#         for quote in quotes:\n",
    "#             text = quote.find('span', class_='text').text\n",
    "#             author = quote.find('small', class_='author').text\n",
    "#             scraped_quotes.append([text,author])\n",
    "#     return scraped_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Pulls only the first tag from each quote\n",
    "# scraped_tags = []\n",
    "# tags = []\n",
    "\n",
    "# def get_tags0(url):\n",
    "#     page_numbers = range(1,11)\n",
    "#     for page in page_numbers:\n",
    "#         soup = get_soup(f'http://quotes.toscrape.com/page/{page}')\n",
    "#         tags = soup.find_all('div', class_ = \"tags\")\n",
    "#         for tag in tags:\n",
    "#             try:\n",
    "#                 tag = tag.find('a').text\n",
    "#                 scraped_tags.append(tag)\n",
    "#             except:\n",
    "#                 print(f\"Missing a tag on page {page}.\")\n",
    "#     return scraped_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tags0 = []\n",
    "# tags0 = get_tags0(url)\n",
    "# tags0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Pulls all tags but with all tag info too\n",
    "\n",
    "# scraped_tags = []\n",
    "# tags = []\n",
    "\n",
    "# def get_tags1(url):\n",
    "#     page_numbers = range(1,11)\n",
    "#     for page in page_numbers:\n",
    "#         soup = get_soup(f'http://quotes.toscrape.com/page/{page}')\n",
    "#         tags = soup.find_all('div', class_ = \"tags\")\n",
    "#         for tag in tags:\n",
    "#             try:\n",
    "#                 tag = tag.find_all('a')\n",
    "#                 scraped_tags.append(tag)\n",
    "#             except:\n",
    "#                 print(f\"Missing a tag on page {page}.\")\n",
    "#     return scraped_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags1 = get_tags1(url)\n",
    "# tags1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping author information\n",
    "\n",
    "def get_author_info(auth_url):\n",
    "    soup = get_soup(auth_url)\n",
    "    author_name = soup.find('h3', class_='author-title').text.strip()\n",
    "    if any(author_name in a for a in scraped_authors):\n",
    "        print(f'Already have info for {author_name}.')\n",
    "    else:\n",
    "        born = soup.find('span', class_='author-born-date').text.strip()\n",
    "        bio = soup.find('div', class_='author-description').text.strip()\n",
    "        bio = bio.replace('\\n', '').replace('        ','')\n",
    "        scraped_authors.append([author_name, born, bio])\n",
    "    return scraped_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling all tags as text - DONE\n",
    "\n",
    "def get_tags2(url):\n",
    "    page_numbers = range(1,11)\n",
    "    for page in page_numbers:\n",
    "        soup = get_soup(f'http://quotes.toscrape.com/page/{page}')\n",
    "        tags = soup.find_all('div', class_ = \"tags\")\n",
    "        for tag in tags:\n",
    "            tag = tag.find_all('a', class_='tag')\n",
    "            for i in tag:\n",
    "                i = i.text\n",
    "                scraped_tags.append(i)\n",
    "    return scraped_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapes Quotes and Authors - DONE\n",
    "# looking for author url to scrape author's bio page - done\n",
    "\n",
    "def get_quotes2(url):\n",
    "    page_numbers = range(1,11)\n",
    "    for page in page_numbers:\n",
    "        soup = get_soup(f'http://quotes.toscrape.com/page/{page}')\n",
    "        quotes = soup.find_all('div', class_='quote')\n",
    "        for quote in quotes:\n",
    "            # finding the author's url\n",
    "            find_auth_url = quote.find('a').get('href')\n",
    "            auth_url = url + find_auth_url\n",
    "            # sending to author defined function\n",
    "            author_info = get_author_info(auth_url)\n",
    "            # finishing the quote scrape\n",
    "            text = quote.find('span', class_='text').text\n",
    "            author = quote.find('small', class_='author').text\n",
    "            scraped_quotes.append([author,text])\n",
    "    return scraped_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_data = get_quotes2(url)\n",
    "quote_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
